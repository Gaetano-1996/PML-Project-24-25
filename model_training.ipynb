{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7312b18-fed3-4c8d-9df2-6bab45923d32",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "e83c2210-e5f2-46c3-aa7d-a4176fcb8bbd",
   "metadata": {},
   "source": [
    "# importing dependencies \n",
    "from utils import *\n",
    "from ddpms import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d8cdf9e-e003-4032-9cb4-1c2af00df061",
   "metadata": {},
   "source": [
    "# Defining general parameters \n",
    "# DDPM's Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Importance sampling specific parameters\n",
    "history_length = 10  # Number of recent values to store"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5431611-d5da-473f-9122-f31fd880be67",
   "metadata": {},
   "source": [
    "# Loading training data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape) / 255),  # Dequantize pixel values\n",
    "    transforms.Lambda(lambda x: (x - 0.5) * 2.0),  # Map from [0,1] -> [-1, -1]\n",
    "    transforms.Lambda(lambda x: x.flatten())\n",
    "])\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c55789e-2f06-4afb-8a67-b2178c426913",
   "metadata": {},
   "source": [
    "# Select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5995746-a374-42bf-9cef-e1c157335d92",
   "metadata": {},
   "source": [
    "model_folder = './model_checkpoints'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b64c3037-13b3-4c0f-8aba-951c2a92ed87",
   "metadata": {},
   "source": [
    "## Provided DDPM"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4a52227-b82f-4776-be3a-de1fc47a3b83",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_classic(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=None)\n",
    "\n",
    "# Save the model \n",
    "#torch.save(model.state_dict(), model_folder+\"/model_classic.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdced432-967a-4080-8fa8-9980b8b54f17",
   "metadata": {},
   "source": [
    "## Low-discrepancy sampling (VDM)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d26a950-3cd8-4bfc-8ef6-6ac186f06d1b",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_low_discrepancy(mnist_unet, T=T, sampler=\"simple\").to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=None)\n",
    "\n",
    "# Save the model \n",
    "#torch.save(model.state_dict(), model_folder+\"/model_lds_simple.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8991bd2-9d0c-4c64-9fc4-e3f8ba4729cb",
   "metadata": {},
   "source": [
    "## Low-discrepancy sampling (Sobol)"
   ]
  },
  {
   "cell_type": "code",
   "id": "fce6f5e7-4937-48d8-b557-42d277657918",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_low_discrepancy(mnist_unet, T=T, sampler=\"sobol\").to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=None)\n",
    "\n",
    "# Save the model \n",
    "#torch.save(model.state_dict(), model_folder+\"/model_lds_sobol.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "966dda48-390e-4cbb-aee6-70c03c5178b5",
   "metadata": {},
   "source": [
    "## Importance sampling "
   ]
  },
  {
   "cell_type": "code",
   "id": "12349606-8112-4329-af15-bdb460e10b3b",
   "metadata": {},
   "source": [
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_importance(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=reporter)\n",
    "\n",
    "# Save the model \n",
    "#torch.save(model.state_dict(), model_folder+\"/model_is.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ccedd33-6c7e-4c79-a170-18621e816faa",
   "metadata": {},
   "source": [
    "## Predicting $x_0$"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9d13884-ef29-4162-83ba-f55e8f85b8d5",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_x0(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=None)\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), model_folder+\"/model_x0.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e95c9e0d-d02b-4545-90cc-9b6e55733538",
   "metadata": {},
   "source": [
    "## Predicting $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "id": "1db04d78-ede5-4ede-b0db-b12681eb4780",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "mnist_unet = ScoreNet2()\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_mu(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=False, per_epoch_callback=reporter)\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), model_folder+\"/model_mu.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26ad4b6d-03cf-415d-ac6f-d3f05a2b3a00",
   "metadata": {},
   "source": [
    "## Classifier Guided"
   ]
  },
  {
   "cell_type": "code",
   "id": "b94864d7-6b72-441e-95a2-a590d2148254",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Classifier specific parameters\n",
    "beta_1 = 1e-4\n",
    "beta_T = 2e-2\n",
    "\n",
    "# Instantiating the classifier\n",
    "model_classifier = RobustMNISTClassifier().to(device)\n",
    "\n",
    "# instantiating the classifier-wrapper\n",
    "wrapper = ClassifierWrapper(model_classifier, T=T, beta_1=beta_1, beta_T=beta_T).to(device)\n",
    "\n",
    "# train the classifier\n",
    "classifier = train_classifier(model_classifier, wrapper)\n",
    "\n",
    "# Saving the classifier\n",
    "torch.save(model_classifier.state_dict(), model_folder + \"/classifier.pt\")\n",
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_class(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train,\n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=None)\n",
    "\n",
    "# No saving of the DDPM model since is the same used in the provided implementation we will load \"/model_classic.pth\" afterwards"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb6f6000-42f1-44cd-ae13-1a4c1f58261d",
   "metadata": {},
   "source": [
    "## Classifier-free Guidance"
   ]
  },
  {
   "cell_type": "code",
   "id": "593b29ba-399a-4f1b-98ef-c93605cdca2f",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Construct model\n",
    "mnist_unet = ScoreNet_class((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM_class_free(mnist_unet, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# Call training loop\n",
    "train_class_free(model, optimizer, scheduler, dataloader_train,\n",
    "                 epochs=epochs, device=device, ema=True, per_epoch_callback=None)\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), model_folder+\"/model_classifier_free.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
